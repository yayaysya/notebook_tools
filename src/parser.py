"""
Markdown è§£æå™¨æ¨¡å—
ä½¿ç”¨ mistune è§£æ MD æ–‡ä»¶,æå–æ–‡æœ¬ã€å›¾ç‰‡å’Œé“¾æ¥
"""
import re
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
import mistune

from config import config
from src.logger_util import setup_logger, log_section, log_list

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger(
    name=__name__,
    log_level=config.LOG_LEVEL,
    log_to_file=config.LOG_TO_FILE,
    log_dir=config.LOG_DIR,
    log_file_prefix=config.LOG_FILE_PREFIX,
    max_size_mb=config.LOG_MAX_SIZE_MB,
    backup_count=config.LOG_BACKUP_COUNT
)


@dataclass
class ParsedContent:
    """è§£æåçš„å†…å®¹ç»“æ„"""
    text_blocks: List[str] = field(default_factory=list)
    code_blocks: List[str] = field(default_factory=list)  # ä»£ç å—
    tags: List[str] = field(default_factory=list)  # #æ ‡ç­¾
    front_matter: Optional[str] = None  # YAML Front Matter
    images: List[Dict[str, str]] = field(default_factory=list)
    links: List[Dict[str, str]] = field(default_factory=list)
    raw_markdown: str = ""


class MarkdownParser:
    """Markdown è§£æå™¨"""

    def __init__(self):
        self.markdown = mistune.create_markdown(
            renderer=None,  # ä½¿ç”¨ AST è€Œä¸æ˜¯ HTML
            plugins=['strikethrough', 'table', 'url']
        )

    def parse(self, markdown_text: str) -> ParsedContent:
        """
        è§£æ Markdown æ–‡æœ¬

        Args:
            markdown_text: Markdown åŸå§‹æ–‡æœ¬

        Returns:
            ParsedContent: è§£æåçš„ç»“æ„åŒ–å†…å®¹
        """
        result = ParsedContent(raw_markdown=markdown_text)

        # æå– YAML Front Matter
        front_matter, content = self._extract_front_matter(markdown_text)
        result.front_matter = front_matter

        # ä½¿ç”¨ mistune è§£æä¸º AST (ä½¿ç”¨å»é™¤ Front Matter åçš„å†…å®¹)
        tokens = self.markdown(content)

        # é€’å½’æå–å†…å®¹
        self._extract_content(tokens, result)

        # é¢å¤–ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ•è·å¯èƒ½é—æ¼çš„å›¾ç‰‡å’Œé“¾æ¥
        self._extract_with_regex(content, result)

        # æå– #æ ‡ç­¾
        result.tags = self._extract_tags(content)

        # å»é‡
        result.images = self._deduplicate_items(result.images, key='url')
        result.links = self._deduplicate_items(result.links, key='url')

        # ============ è¯¦ç»†æ—¥å¿— ============
        if logger.isEnabledFor(logging.DEBUG):
            log_section(logger, "ğŸ“„ Markdown è§£æç»“æœ")

            if result.front_matter:
                logger.debug("Front Matter (YAML):")
                logger.debug(f"```yaml\n{result.front_matter}\n```\n")

            logger.debug(f"åŸå§‹æ–‡ä»¶å¤§å°: {len(markdown_text)} å­—ç¬¦")
            logger.debug(f"æ–‡æœ¬å—æ•°é‡: {len(result.text_blocks)}")

            for i, block in enumerate(result.text_blocks, 1):
                preview = block[:100].replace('\n', '\\n')
                logger.debug(f"  æ–‡æœ¬å—[{i}]: {preview}{'...' if len(block) > 100 else ''}")

            logger.debug(f"\nä»£ç å—æ•°é‡: {len(result.code_blocks)}")
            for i, code in enumerate(result.code_blocks, 1):
                logger.debug(f"  ä»£ç å—[{i}]:\n{code}\n")

            logger.debug(f"å›¾ç‰‡æ•°é‡: {len(result.images)}")
            for i, img in enumerate(result.images, 1):
                logger.debug(f"  å›¾ç‰‡[{i}]: {img['url']} (alt: {img['alt']})")

            logger.debug(f"\né“¾æ¥æ•°é‡: {len(result.links)}")
            for i, link in enumerate(result.links, 1):
                logger.debug(f"  é“¾æ¥[{i}]: {link['title']} -> {link['url']}")

            logger.debug(f"\næ ‡ç­¾: {', '.join(['#' + tag for tag in result.tags]) if result.tags else '(æ— )'}")
            log_section(logger, "", char="=")

        return result

    def _extract_front_matter(self, markdown_text: str) -> Tuple[Optional[str], str]:
        """
        æå– YAML Front Matter

        Args:
            markdown_text: Markdown æ–‡æœ¬

        Returns:
            (front_matter, remaining_text): å…ƒæ•°æ®å’Œå‰©ä½™å†…å®¹
        """
        # åŒ¹é…å¼€å¤´çš„ ---\n...\n---
        pattern = r'^---\s*\n(.*?)\n---\s*\n'
        match = re.match(pattern, markdown_text, re.DOTALL)

        if match:
            front_matter = match.group(1).strip()
            remaining_text = markdown_text[match.end():]
            logger.debug(f"æ£€æµ‹åˆ° YAML Front Matter ({len(front_matter)} å­—ç¬¦)")
            return front_matter, remaining_text

        return None, markdown_text

    def _extract_content(self, tokens: List, result: ParsedContent, context: str = ""):
        """é€’å½’æå– tokens ä¸­çš„å†…å®¹"""
        for token in tokens:
            token_type = token.get('type', '')

            if token_type == 'paragraph':
                # æå–æ®µè½æ–‡æœ¬
                text = self._extract_text(token)
                if text.strip():
                    result.text_blocks.append(text.strip())
                    context = text.strip()[:100]  # ä¿ç•™å‰100å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡

            elif token_type == 'heading':
                # æå–æ ‡é¢˜æ–‡æœ¬
                text = self._extract_text(token)
                if text.strip():
                    result.text_blocks.append(f"# {text.strip()}")

            elif token_type == 'list':
                # æå–åˆ—è¡¨æ–‡æœ¬
                text = self._extract_text(token)
                if text.strip():
                    result.text_blocks.append(text.strip())

            elif token_type == 'block_code':
                # æå–ä»£ç å—,ä¿ç•™åŸå§‹æ ¼å¼
                code = token.get('raw', '').strip()
                lang = token.get('info', '')
                code_block = f"```{lang}\n{code}\n```"
                result.code_blocks.append(code_block)
                result.text_blocks.append(code_block)  # åŒæ—¶åŠ å…¥æ–‡æœ¬å—

            elif token_type == 'block_quote':
                # æå–å¼•ç”¨å—
                text = self._extract_text(token)
                if text.strip():
                    quote_block = f"> {text.strip()}"
                    result.text_blocks.append(quote_block)

            elif token_type == 'thematic_break':
                # åˆ†éš”çº¿
                result.text_blocks.append("---")

            elif token_type == 'image':
                # æå–å›¾ç‰‡
                url = token.get('src', '')
                alt = token.get('alt', '')
                if url:
                    result.images.append({
                        'url': url,
                        'alt': alt,
                        'context': context
                    })

            elif token_type == 'link':
                # æå–é“¾æ¥
                url = token.get('link', '')
                title = self._extract_text(token)
                if url and self._is_webpage_url(url):
                    result.links.append({
                        'url': url,
                        'title': title,
                        'context': context
                    })

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in token:
                self._extract_content(token['children'], result, context)

    def _extract_text(self, token: Dict) -> str:
        """ä» token ä¸­æå–çº¯æ–‡æœ¬"""
        if token.get('type') == 'text':
            return token.get('raw', '')

        # å¤„ç†è¡Œå†…ä»£ç  (codespan)
        if token.get('type') == 'codespan':
            code = token.get('raw', '')
            return f"`{code}`"  # ä¿ç•™åå¼•å·æ ¼å¼

        text_parts = []
        if 'children' in token:
            for child in token['children']:
                text_parts.append(self._extract_text(child))

        return ' '.join(text_parts)

    def _extract_with_regex(self, markdown_text: str, result: ParsedContent):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¡¥å……æå–å›¾ç‰‡å’Œé“¾æ¥"""

        # æå–å›¾ç‰‡: ![alt](url)
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        for match in re.finditer(image_pattern, markdown_text):
            alt, url = match.groups()
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not any(img['url'] == url for img in result.images):
                result.images.append({
                    'url': url,
                    'alt': alt,
                    'context': self._get_surrounding_text(markdown_text, match.start())
                })

        # æå–é“¾æ¥: [text](url)
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        for match in re.finditer(link_pattern, markdown_text):
            title, url = match.groups()
            if self._is_webpage_url(url) and not any(link['url'] == url for link in result.links):
                result.links.append({
                    'url': url,
                    'title': title,
                    'context': self._get_surrounding_text(markdown_text, match.start())
                })

    def _get_surrounding_text(self, text: str, position: int, radius: int = 100) -> str:
        """è·å–æŒ‡å®šä½ç½®å‘¨å›´çš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡"""
        start = max(0, position - radius)
        end = min(len(text), position + radius)
        context = text[start:end].strip()
        # ç§»é™¤ markdown è¯­æ³•
        context = re.sub(r'[#*_\[\]()!]', '', context)
        return context[:100]

    def _extract_tags(self, markdown_text: str) -> List[str]:
        """
        æå– #æ ‡ç­¾

        Args:
            markdown_text: Markdown æ–‡æœ¬

        Returns:
            List[str]: æ ‡ç­¾åˆ—è¡¨(ä¸å« # ç¬¦å·)
        """
        # åŒ¹é… #æ ‡ç­¾ æ ¼å¼ (æ”¯æŒä¸­è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿)
        # ç¡®ä¿ # å‰é¢æ˜¯ç©ºç™½æˆ–è¡Œé¦–,é¿å…è¯¯åŒ¹é… Markdown æ ‡é¢˜
        pattern = r'(?:^|[^\w#])(#[\w\u4e00-\u9fa5_]+)'
        matches = re.findall(pattern, markdown_text, re.MULTILINE)

        # å»é™¤ # ç¬¦å·å¹¶å»é‡(ä¿æŒé¡ºåº)
        seen = set()
        unique_tags = []
        for match in matches:
            tag = match.lstrip('#')
            # æ’é™¤çº¯æ•°å­—(é¿å…è¯¯åŒ¹é…)
            if tag and not tag.isdigit() and tag not in seen:
                seen.add(tag)
                unique_tags.append(tag)

        return unique_tags

    @staticmethod
    def _is_webpage_url(url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç½‘é¡µ URL (æ’é™¤å›¾ç‰‡ã€é”šç‚¹ç­‰)"""
        if not url:
            return False

        # æ’é™¤é”šç‚¹é“¾æ¥
        if url.startswith('#'):
            return False

        # æ’é™¤å¸¸è§å›¾ç‰‡æ ¼å¼
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
        if any(url.lower().endswith(ext) for ext in image_extensions):
            return False

        # å¿…é¡»æ˜¯ http/https åè®®
        return url.startswith('http://') or url.startswith('https://')

    @staticmethod
    def _deduplicate_items(items: List[Dict], key: str) -> List[Dict]:
        """æ ¹æ®æŒ‡å®š key å»é‡"""
        seen = set()
        unique_items = []
        for item in items:
            value = item.get(key)
            if value and value not in seen:
                seen.add(value)
                unique_items.append(item)
        return unique_items


def parse_markdown_file(file_path: str) -> ParsedContent:
    """
    è§£æ Markdown æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°

    Args:
        file_path: MD æ–‡ä»¶è·¯å¾„

    Returns:
        ParsedContent: è§£æç»“æœ
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    parser = MarkdownParser()
    return parser.parse(content)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_md = """
# æµ‹è¯•æ ‡é¢˜

è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ã€‚

![æµ‹è¯•å›¾ç‰‡](https://example.com/image.jpg)

è¿™é‡Œæœ‰ä¸€ä¸ª[æµ‹è¯•é“¾æ¥](https://example.com/article)ã€‚

- åˆ—è¡¨é¡¹ 1
- åˆ—è¡¨é¡¹ 2
"""

    parser = MarkdownParser()
    result = parser.parse(test_md)

    print("æ–‡æœ¬å—:", result.text_blocks)
    print("å›¾ç‰‡:", result.images)
    print("é“¾æ¥:", result.links)
